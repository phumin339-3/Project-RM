# train_model.py
import os, json, joblib
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use("Agg")  # ‡πÉ‡∏ä‡πâ backend ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á GUI
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, accuracy_score, confusion_matrix,
    precision_recall_curve, roc_auc_score, average_precision_score,
    roc_curve, f1_score, precision_score, recall_score
)
from sklearn.inspection import permutation_importance

# ================== CONFIG ==================
TARGET_RECALL = 0.90                          # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ recall ‡∏Ç‡∏≠‡∏á class=unsafe (label=1)
THRESH_GRID   = np.linspace(0.0, 1.0, 201)    # ‡∏™‡πÅ‡∏Å‡∏ô threshold 0..1 step=0.005

# ================== Paths ==================
base_dir   = os.path.abspath(os.path.join(os.path.dirname(__file__), "../Feature_ML"))
phish_path = os.path.join(base_dir, "phishing_urls.csv")
legit_path = os.path.join(base_dir, "legitimate_urls.csv")

plot_dir = os.path.abspath(os.path.join(os.getcwd(), "python/plots"))
os.makedirs(plot_dir, exist_ok=True)

# ================== Load ==================
phish_df = pd.read_csv(phish_path)
legit_df = pd.read_csv(legit_path)

# ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå label ‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏õ‡πâ‡∏≤‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (1=unsafe, 0=safe)
if "label" not in phish_df.columns:
    phish_df["label"] = 1
if "label" not in legit_df.columns:
    legit_df["label"] = 0

df_raw = pd.concat([phish_df, legit_df], ignore_index=True)

# ================== Base Features (‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö app.py/url_checker.py) ==================
base_features = [
    # URL-level
    "length_url","length_hostname","ip","punycode","ratio_digits_url","ratio_digits_host",
    "nb_subdomains","tld_in_path","tld_in_subdomain","random_domain","shortening_service",
    "prefix_suffix","phish_hints","url_entropy","uses_https","is_http","num_query_params",
    "has_at_symbol","path_extension","tld_risk",
    # TLS/WHOIS/HTTP
    "ssl_valid","cert_days_left","san_count","cert_cn_matches_domain","domain_age_days",
    "days_since_update","has_whois_privacy","redirect_chain_len","final_domain_differs",
    "has_security_headers",
    # Typosquat (light)
    "typosquat_candidate",
    # Similarity scores (‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô dataset ‚Üí ‡πÄ‡∏ï‡∏¥‡∏° 0 ‡πÉ‡∏´‡πâ)
    "typosquat_score_max","typosquat_score_mean","typosquat_distance",
    # Target
    "label"
]

# ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ default ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
defaults = {
    # URL-level
    "length_url":0,"length_hostname":0,"ip":0,"punycode":0,"ratio_digits_url":0.0,"ratio_digits_host":0.0,
    "nb_subdomains":0,"tld_in_path":0,"tld_in_subdomain":0,"random_domain":0,"shortening_service":0,
    "prefix_suffix":0,"phish_hints":0,"url_entropy":0.0,"uses_https":0,"is_http":0,"num_query_params":0,
    "has_at_symbol":0,"path_extension":0,"tld_risk":0,
    # TLS/WHOIS/HTTP
    "ssl_valid":0,"cert_days_left":0,"san_count":0,"cert_cn_matches_domain":0,"domain_age_days":-1,
    "days_since_update":0,"has_whois_privacy":0,"redirect_chain_len":0,"final_domain_differs":0,
    "has_security_headers":0,
    # Typosquat
    "typosquat_candidate":0,
    # Similarity
    "typosquat_score_max":0,"typosquat_score_mean":0,"typosquat_distance":0,
}

for col in base_features:
    if col == "label":
        continue
    if col not in df_raw.columns:
        df_raw[col] = defaults.get(col, 0)

# ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö base_features
df = df_raw[[c for c in base_features if c in df_raw.columns]].copy()

# ================== Extra Feature Engineering (‡∏ï‡πâ‡∏≠‡∏á ‚Äú‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‚Äù ‡∏Å‡∏±‡∏ö app.py) ==================
# app.py ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì 5 ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡∏ï‡∏≠‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü ‚Üí ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô feature_order.json
df["url_length_ratio"] = df["length_url"] / (df["length_hostname"] + 1)
df["digit_ratio_diff"] = (df["ratio_digits_url"] - df["ratio_digits_host"]).abs()
df["domain_age_lt_90d"] = ((df["domain_age_days"] >= 0) & (df["domain_age_days"] < 90)).astype(int)
df["ssl_invalid_or_short"] = ((df["ssl_valid"] == 0) | (df["cert_days_left"] < 14)).astype(int)
df["redirect_and_domain_diff"] = ((df["redirect_chain_len"] > 0) & (df["final_domain_differs"] == 1)).astype(int)

# ================== Clean & Split ==================
X = df.drop("label", axis=1).copy()
y = df["label"].astype(int)  # 1 = unsafe, 0 = safe

# ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö numeric + ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á/inf
X = X.apply(pd.to_numeric, errors="coerce")
X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

# ---- NEW: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡∏Å‡πà‡∏≠‡∏ô split ----
value_counts = y.value_counts().to_dict()
print("Class counts:", value_counts)  # debug ‡∏î‡∏π‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™

min_class = min(value_counts.values()) if len(value_counts) > 0 else 0

# ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡πÉ‡∏ä‡πâ stratify:
# - ‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
# - ‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏∏‡∏î >= 2
# - ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ó‡∏™‡πÄ‡∏ã‡πá‡∏ï‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏∏‡∏î >= 1 (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ)
TEST_SIZE = 0.2
use_stratify = (
    set(value_counts.keys()) == {0, 1} and
    min_class >= 2 and
    int(round(min_class * TEST_SIZE)) >= 1
)

if use_stratify:
    stratify_arg = y
    print("Split mode: stratified")
else:
    stratify_arg = None
    print("Split mode: NON-stratified (fallback, because minority class too small)")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, stratify=stratify_arg, random_state=42
)


# ================== Train RandomForest ==================
clf = RandomForestClassifier(
    n_estimators=1200,
    max_depth=None,
    min_samples_split=4,
    min_samples_leaf=1,
    max_features="sqrt",
    bootstrap=True,
    oob_score=True,
    max_samples=0.9,
    class_weight={0: 1.0, 1: 1.3},  # 1 = unsafe (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î false negative)
    random_state=42,
    n_jobs=-1
)
clf.fit(X_train, y_train)
print("OOB score:", round(clf.oob_score_, 4))

# ================== Evaluate @0.5 (robust to single-class) ==================
# ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢ y_train ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‚Üí predict_proba ‡∏à‡∏∞‡∏°‡∏µ 1 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
proba_all = clf.predict_proba(X_test)

if proba_all.shape[1] == 1:
    # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    only_class = int(clf.classes_[0])
    if only_class == 1:
        # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ñ‡∏∑‡∏≠ "unsafe" ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ proba_unsafe = ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤
        proba = proba_all[:, 0]
    else:
        # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ñ‡∏∑‡∏≠ "safe" ‡πÉ‡∏´‡πâ proba_unsafe = 0 ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        proba = np.zeros(len(X_test), dtype=float)
else:
    # ‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö‡∏™‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™ ‚Üí ‡∏´‡∏≤ index ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™ 1 (unsafe)
    unsafe_idx = int(np.where(clf.classes_ == 1)[0][0])
    proba = proba_all[:, unsafe_idx]

pred_05 = (proba >= 0.5).astype(int)
print("\nüéØ Accuracy @0.5:", round(accuracy_score(y_test, pred_05), 4))
print("\nüìä Report @0.5:\n",
      classification_report(y_test, pred_05, target_names=["safe","unsafe"], digits=4))


# ================== Threshold Scan (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å F1 ‡∏î‡∏µ‡∏™‡∏∏‡∏î‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ recall ‚â• TARGET_RECALL) ==================
scan_rows = []
for thr in THRESH_GRID:
    pred = (proba >= thr).astype(int)
    prec1 = precision_score(y_test, pred, pos_label=1, zero_division=0)
    rec1  = recall_score(y_test, pred, pos_label=1, zero_division=0)
    f1_1  = f1_score(y_test, pred, pos_label=1, zero_division=0)
    acc   = accuracy_score(y_test, pred)
    scan_rows.append({"threshold": float(thr),
                      "precision_unsafe": float(prec1),
                      "recall_unsafe": float(rec1),
                      "f1_unsafe": float(f1_1),
                      "accuracy": float(acc)})

scan_df = pd.DataFrame(scan_rows)
scan_csv_path = os.path.join(plot_dir, "threshold_scan.csv")
scan_df.to_csv(scan_csv_path, index=False)

# ‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà recall >= TARGET_RECALL ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å f1 ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
candidates = scan_df[scan_df["recall_unsafe"] >= TARGET_RECALL]
if len(candidates) == 0:
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∏‡∏î‡πÑ‡∏´‡∏ô‡∏ó‡∏≥ recall ‡∏ñ‡∏∂‡∏á target ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡∏ó‡∏µ‡πà recall ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏î‡∏π f1/precision
    best_row = scan_df.sort_values(["recall_unsafe","f1_unsafe","precision_unsafe"], ascending=[False, False, False]).iloc[0]
else:
    best_row = candidates.sort_values(["f1_unsafe","precision_unsafe","accuracy"], ascending=[False, False, False]).iloc[0]

best_thr = float(best_row["threshold"])
print(f"\nüîß Tuned threshold (recall‚â•{TARGET_RECALL}, best F1_unsafe): {best_thr:.3f}")

tuned_pred = (proba >= best_thr).astype(int)
print("\nüìä Report @tuned:\n",
      classification_report(y_test, tuned_pred, target_names=["safe","unsafe"], digits=4))

# ================== Curves & Plots ==================
# PR curve (global) ‚Äî AP ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ y_test ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÑ‡∏ß‡πâ
ap = average_precision_score(y_test, proba)
prec_curve, rec_curve, _ = precision_recall_curve(y_test, proba)
plt.figure()
plt.plot(rec_curve, prec_curve)
plt.xlabel("Recall"); plt.ylabel("Precision")
plt.title(f"Precision-Recall (AP={ap:.3f})")
plt.tight_layout()
plt.savefig(os.path.join(plot_dir, "pr_curve.png"))
plt.close()

# ROC ‚Äî ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô y_test
if len(np.unique(y_test)) == 2:
    fpr, tpr, _ = roc_curve(y_test, proba)
    roc_auc = roc_auc_score(y_test, proba)
    plt.figure()
    plt.plot(fpr, tpr)
    plt.xlabel("FPR"); plt.ylabel("TPR")
    plt.title(f"ROC (AUC={roc_auc:.3f})")
    plt.tight_layout()
    plt.savefig(os.path.join(plot_dir, "roc_curve.png"))
    plt.close()
else:
    roc_auc = float("nan")
    print("‚ö†Ô∏è ROC AUC skipped: y_test ‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß")

# Confusion @ tuned (‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
cm = confusion_matrix(y_test, tuned_pred)
...

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["safe","unsafe"], yticklabels=["safe","unsafe"])
plt.xlabel("Predicted"); plt.ylabel("True")
plt.title(f"Confusion Matrix (thr={best_thr:.3f})")
plt.tight_layout()
plt.savefig(os.path.join(plot_dir, "confusion_matrix_tuned.png"))
plt.close()

# Feature Importances (Tree-based)
importances = clf.feature_importances_
indices = np.argsort(importances)[::-1]
topk = min(30, len(indices))
plt.figure(figsize=(12,6))
sns.barplot(x=importances[indices][:topk], y=X.columns[indices][:topk])
plt.title("Top 30 Feature Importances (RF)")
plt.tight_layout()
plt.savefig(os.path.join(plot_dir, "feature_importance.png"))
plt.close()

# Permutation Importance
perm = permutation_importance(clf, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)
pi_sorted = np.argsort(perm.importances_mean)[::-1][:topk]
plt.figure(figsize=(12,6))
sns.barplot(x=perm.importances_mean[pi_sorted], y=X.columns[pi_sorted])
plt.title("Top 30 Permutation Importances")
plt.tight_layout()
plt.savefig(os.path.join(plot_dir, "perm_importance.png"))
plt.close()

# ================== Helper: ‡∏™‡∏£‡πâ‡∏≤‡∏á "‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û" ==================
def save_report_table(y_true, y_pred, out_png_path, digits=2):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (precision/recall/f1/support + accuracy, macro avg, weighted avg)
    """
    rep = classification_report(
        y_true, y_pred, target_names=["safe","unsafe"], digits=digits, output_dict=True
    )

    rows_order = ["safe", "unsafe", "accuracy", "macro avg", "weighted avg"]
    cols_order = ["precision", "recall", "f1-score", "support"]

    table_data = []
    for r in rows_order:
        if r == "accuracy":
            acc = rep["accuracy"]
            row = [acc, acc, acc, rep["safe"]["support"] + rep["unsafe"]["support"]]
        else:
            row = [
                rep[r]["precision"],
                rep[r]["recall"],
                rep[r]["f1-score"],
                rep[r]["support"],
            ]
        table_data.append(row)

    df_report = pd.DataFrame(table_data, index=rows_order, columns=cols_order)
    df_report[["precision","recall","f1-score"]] = df_report[["precision","recall","f1-score"]].astype(float).round(digits)
    df_report["support"] = df_report["support"].astype(float).round(1)

    fig, ax = plt.subplots(figsize=(7, 2.6))
    ax.axis('off')
    tbl = ax.table(
        cellText=df_report.values,
        rowLabels=df_report.index,
        colLabels=df_report.columns,
        loc='center',
        cellLoc='center',
        rowLoc='center'
    )
    tbl.auto_set_font_size(False)
    tbl.set_fontsize(10)
    tbl.scale(1, 1.2)
    plt.tight_layout()
    plt.savefig(out_png_path, dpi=200, bbox_inches='tight')
    plt.close()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á threshold
save_report_table(y_test, pred_05,   os.path.join(plot_dir, "classification_report_0p5.png"))
save_report_table(y_test, tuned_pred, os.path.join(plot_dir, "classification_report_tuned.png"))
print(f"‚úÖ Saved report tables to {plot_dir}")

# ================== Save model + feature order + metrics ==================
model_dir = os.path.dirname(__file__)
model_path = os.path.join(model_dir, "phishing_model.pkl")
joblib.dump(clf, model_path)

# ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå EXACT ‡∏ï‡∏≤‡∏° X.columns ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ app.py ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô build_feature_row
with open(os.path.join(model_dir, "feature_order.json"), "w", encoding="utf-8") as f:
    json.dump(list(X.columns), f, ensure_ascii=False, indent=2)

metrics_path = os.path.join(model_dir, "metrics.json")
with open(metrics_path, "w", encoding="utf-8") as f:
    json.dump({
        "oob_score": float(clf.oob_score_),
        "accuracy@0.5": float(accuracy_score(y_test, pred_05)),
        "roc_auc": float(roc_auc),
        "avg_precision": float(ap),
        "tuned_threshold": float(best_thr),
        "tuned_report": classification_report(
            y_test, tuned_pred, target_names=["safe","unsafe"], digits=4, output_dict=True
        ),
        "target_recall": float(TARGET_RECALL),
        "threshold_scan_csv": scan_csv_path
    }, f, ensure_ascii=False, indent=2)

print(f"\n‚úÖ Model saved to {model_path}")
print(f"‚úÖ Metrics saved to {metrics_path}")
print(f"‚úÖ Threshold scan saved to {scan_csv_path}")
print(f"‚úÖ Plots saved to: {plot_dir}")
